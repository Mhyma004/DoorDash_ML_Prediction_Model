{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing datasets\n",
    "historical = pd.read_csv(\"Downloads/historical_data.csv\", usecols=['created_at', 'actual_delivery_time', \n",
    "                                                                   'estimated_order_place_duration',\n",
    "                                                                   'estimated_store_to_consumer_driving_duration'])\n",
    "predict    = pd.read_csv(\"OneDrive/Desktop/predict_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling NA with mean if column\n",
    "historical[\"estimated_store_to_consumer_driving_duration\"].fillna(historical.estimated_store_to_consumer_driving_duration.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unneeded columns\n",
    "predict.drop(['market_id','store_id', 'store_primary_category',\n",
    "       'order_protocol', 'total_items', 'subtotal', 'num_distinct_items',\n",
    "       'min_item_price', 'max_item_price', 'total_onshift_dashers',\n",
    "       'total_busy_dashers', 'total_outstanding_orders',\n",
    "       'platform','delivery_id'], axis = 1, inplace=True)\n",
    "\n",
    "#filling NA with mean in column\n",
    "predict[\"estimated_store_to_consumer_driving_duration\"].fillna(predict.estimated_store_to_consumer_driving_duration.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limiting training dataset\n",
    "historical = historical.sample(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "#converting date to timestamp \n",
    "historical['created_at'] = pd.to_datetime(historical['created_at'])\n",
    "historical['created_at'] = historical['created_at'].map(dt.datetime.timestamp)\n",
    "\n",
    "historical['actual_delivery_time'] = pd.to_datetime(historical['actual_delivery_time'])\n",
    "historical['actual_delivery_time'] = historical['actual_delivery_time'].map(dt.datetime.timestamp)\n",
    "\n",
    "predict['created_at'] = pd.to_datetime(predict['created_at'])\n",
    "predict['created_at'] = predict['created_at'].map(dt.datetime.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data for fitting \n",
    "train = historical.drop(['actual_delivery_time'], axis = 1)\n",
    "test = historical['actual_delivery_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train, test,shuffle = True, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training model\n",
    "regr = LinearRegression()\n",
    "regr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing model \n",
    "x_predict = regr.predict(x_test)\n",
    "\n",
    "#showing models performance on test data\n",
    "RMSE = mean_squared_error( y_test, x_predict, squared= False)\n",
    "MAE  =  mean_absolute_error(y_test, x_predict)\n",
    "rsquared = r2_score(y_test, x_predict)\n",
    "MSE = mean_squared_error(y_test, x_predict)\n",
    "MAE = mean_absolute_error(y_test, x_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  1070.5496599968158\n",
      "MAE:   773.6635555307269\n",
      "R2:    0.9999976477482351\n",
      "MSE:   1146076.574519298\n",
      "MAE:   773.6635555307269\n"
     ]
    }
   ],
   "source": [
    "#printing metrics\n",
    "print (\"RMSE: \",  RMSE)\n",
    "print (\"MAE:  \",  MAE)\n",
    "print (\"R2:   \",  rsquared)\n",
    "print (\"MSE:  \",  MSE)\n",
    "print (\"MAE:  \",  MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        3249.873951\n",
       "1        2973.324200\n",
       "2        3034.626931\n",
       "3        3270.729143\n",
       "4        3071.616773\n",
       "            ...     \n",
       "54773    3171.752884\n",
       "54774    2747.494287\n",
       "54775    2550.917838\n",
       "54776    2884.053860\n",
       "54777    2976.411707\n",
       "Length: 54778, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting on new dataset\n",
    "predicted_delivery_time_regr = regr.predict(predict)\n",
    "\n",
    "#placing prediction into dataframe\n",
    "df = pd.DataFrame(predicted_delivery_time_regr, columns=['predicted_delivery_time'])\n",
    "\n",
    "#converting back to datetine\n",
    "df = df['predicted_delivery_time'].map(dt.datetime.fromtimestamp)\n",
    "df2 = predict['created_at'].map(dt.datetime.fromtimestamp)\n",
    "\n",
    "#taking the duration\n",
    "td = df-df2\n",
    "\n",
    "#conveting into seconds\n",
    "td.dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to prediction dataset\n",
    "predict = pd.read_csv(\"OneDrive/Desktop/predict_data.csv\")\n",
    "predict['predicted_duration'] = td.dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output to csv\n",
    "predict.to_csv(path_or_buf = \"OneDrive/Desktop/predicted_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training model\n",
    "dt_reg = tree.DecisionTreeRegressor(max_depth=5)\n",
    "dt_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing model \n",
    "x_predict = dt_reg.predict(x_test)\n",
    "\n",
    "#showing models performance on test data\n",
    "RMSE = mean_squared_error( y_test, x_predict, squared= False)\n",
    "MAE  =  mean_absolute_error(y_test, x_predict)\n",
    "rsquared = r2_score(y_test, x_predict)\n",
    "MSE = mean_squared_error(y_test, x_predict)\n",
    "MAE = mean_absolute_error(y_test, x_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  11507.167119321259\n",
      "MAE:   9654.006444534778\n",
      "R2:    0.9997282265621248\n",
      "MSE:   132414895.1119883\n",
      "MAE:   9654.006444534778\n"
     ]
    }
   ],
   "source": [
    "#printing metrics\n",
    "print (\"RMSE: \",  RMSE)\n",
    "print (\"MAE:  \",  MAE)\n",
    "print (\"R2:   \",  rsquared)\n",
    "print (\"MSE:  \",  MSE)\n",
    "print (\"MAE:  \",  MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training model\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing model \n",
    "x_predict = rf_reg.predict(x_test)\n",
    "\n",
    "#showing models performance on test data\n",
    "RMSE = mean_squared_error( y_test, x_predict, squared= False)\n",
    "MAE  =  mean_absolute_error(y_test, x_predict)\n",
    "rsquared = r2_score(y_test, x_predict)\n",
    "MSE = mean_squared_error(y_test, x_predict)\n",
    "MAE = mean_absolute_error(y_test, x_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  1117.4812076870512\n",
      "MAE:   805.4081376290321\n",
      "R2:    0.9999974369880903\n",
      "MSE:   1248764.2495337105\n",
      "MAE:   805.4081376290321\n"
     ]
    }
   ],
   "source": [
    "#printing metrics\n",
    "print (\"RMSE: \",  RMSE)\n",
    "print (\"MAE:  \",  MAE)\n",
    "print (\"R2:   \",  rsquared)\n",
    "print (\"MSE:  \",  MSE)\n",
    "print (\"MAE:  \",  MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training model\n",
    "gb_reg = ensemble.GradientBoostingRegressor()\n",
    "gb_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing model \n",
    "x_predict = gb_reg.predict(x_test)\n",
    "\n",
    "#showing models performance on test data\n",
    "RMSE = mean_squared_error( y_test, x_predict, squared= False)\n",
    "MAE  =  mean_absolute_error(y_test, x_predict)\n",
    "rsquared = r2_score(y_test, x_predict)\n",
    "MSE = mean_squared_error(y_test, x_predict)\n",
    "MAE = mean_absolute_error(y_test, x_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  3310.8619441546725\n",
      "MAE:   2286.7321946053503\n",
      "R2:    0.9999775015649075\n",
      "MSE:   10961806.81325166\n",
      "MAE:   2286.7321946053503\n"
     ]
    }
   ],
   "source": [
    "#printing metrics\n",
    "print (\"RMSE: \",  RMSE)\n",
    "print (\"MAE:  \",  MAE)\n",
    "print (\"R2:   \",  rsquared)\n",
    "print (\"MSE:  \",  MSE)\n",
    "print (\"MAE:  \",  MAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
